## 概要

・本プログラムは、googleロケーション履歴から得られる緯度経度、日時を元に逆強化学習と呼ばれる対象者の軌道から報酬を算出する手法を用いて計算した結果から、強化学習を行い移動データを再現するプログラムである。<br>
このリポジトリには、以下のフォルダが含まれています。<br>

・事前データの準備-->googleロケーション履歴をjson形式でダウンロードしたものをフォルダ内に配置することで、jsonファイル内から日時,緯度,軽度を抽出しデータベースファイルに変換する。<br>
・移動モデルの逆強化学習-->事前データの準備で作成したデータベースファイルを元に1時間ごと(0時から23時)で逆強化学習を行う。<br>
・移動モデルの強化学習-->移動モデルの逆強化学習で計算した結果(excelファイル)を入力として使用しQ学習を用いた強化学習を行う。<br>

## 実験の流れ

・googleロケーション履歴をダウンロード
    https://takeout.google.com/?pli=1
    から、ロケーション履歴をjson形式でダウンロードする。(選択肢に出てこない場合機能を有効にしていない可能性があるため、以下を参考に有効化する)
    https://support.google.com/accounts/answer/3118687?hl=ja#:~:text=%E3%83%AD%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E5%B1%A5%E6%AD%B4%E3%82%92%E3%82%AA%E3%83%B3%E3%81%BE%E3%81%9F%E3%81%AF%E3%82%AA%E3%83%95%E3%81%AB%E3%81%99%E3%82%8B&text=Google%20%E3%82%A2%E3%82%AB%E3%82%A6%E3%83%B3%E3%83%88%E3%81%AE%20%5B%E3%83%AD%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E5%B1%A5%E6%AD%B4,%E3%82%AA%E3%83%B3%E3%81%BE%E3%81%9F%E3%81%AF%E3%82%AA%E3%83%95%E3%81%AB%E3%81%97%E3%81%BE%E3%81%99%E3%80%82

・ダウンロードしたロケーション履歴をデータベースファイルに変換
  ダウンロードしたjsonファイルを、事前データの準備フォルダ内に移動しjson2db.pyを実行する。<br>
  
  python json2db.py
  
  実行後データベースファイルがが生成または更新される。<br>

・逆強化学習を実行
  jsonから変換したデータベースファイルを、移動モデルの逆強化学習フォルダ内に移動しstart.bat(windows専用)を実行する。この時main.py内の30.31行目の
```        state_features = np.vstack([self.state_to_feature2(s)for s in range(self.n_states)])```

  の"self.state_to_feature"が"self.state_to_feature"であれば、報酬=thetaとして計算する。また、self.state_to_feature2"に変更することで、報酬=theta×fs(平均訪問回数)として逆強化学習を実行する。<br>
  
  start.batを起動後は、実験名を入力しEnterを押すことで24個のコマンドプロンプロが起動し各時間に対して逆強化学習が行われる。<br>
  
  start.bat
  実験名を入力を入力しEnter

  すべての処理が終了すると./移動モデルの逆強化学習/result/実験名 でフォルダが生成されているため、その中から各時間のエクセルファイルを抜き出し１つのフォルダに格納する。<br>

・強化学習を実行
  逆強化学習の結果をまとめたフォルダを./移動モデルの強化学習/input_data内にフォルダを移動させる。
  移動後、main.dbを削除もしくは初期化する。<br>
  #本プログラムでは、移動経路を報酬の値ごとに足切りをし短時間で報酬が一定以上の値間を結ぶ最短経路を結んでいる。この時移動先と出発地点を決定後、各地点から最短経路を計算すると報酬の少ない地点を選択する可能性や斜めに移動せず直線的な移動経路となってしまうため、報酬をネットワークグラフに変換し報酬が基準値より低いノードをネットワークから切り離し最短経路を計算することで最短経路を計算した場合にすべての地点を選択肢に入れた最短経路探索で得られる報酬以上の報酬を獲得することができる。<br>
  以上の処理を実行するために./移動モデルの強化学習/network.pyを実行し各地点間の経路を決定し保存する。<br>
  計算した経路を./移動モデルの強化学習/main.pyを実行し強化学習をする。<br>
  実行後は、./移動モデルの強化学習/main.db内に[id,スタート地点,ゴール地点,報酬,step数,移動ルート]が保存され./移動モデルの強化学習/step_reward.pngには１エピソードを1440(60分*24時間)ステップと仮定した場合の累積報酬が記録される。<br>
  この結果を、./移動モデルの強化学習/visualization.pyを実行しmp4ファイルとして保存することによって可視化する。<br>

## プログラムの解説
./事前データの準備/jspm2db.py-->jsonファイルからデータを抽出しデータベールファイルに保存<br>

./移動モデルの逆強化学習/main.py-->逆強化学習を行うメインプログラム。報酬の計算や状態価値関数を用いて得られたポリシーとエキスパートの方策の差を計算するなどを行っている。<br>
./移動モデルの逆強化学習/db2expart.py-->データベースファイルからエキスパートの軌道をグリッド形式で読み込み値を返す。<br>
./移動モデルの逆強化学習/value_interation.py-->入力された報酬から予測される方策(Policy)を計算する。<br>

./移動モデルの強化学習/network.py-->逆強化学習で計算された結果をもとに、訪問地点を決定し各訪問地点間のルートと報酬を記録する。<br>
./移動モデルの強化学習/main.py-->強化学習を実行するためのメインプログラム。<br>
./移動モデルの強化学習/qagent.py-->Qテーブルを更新するためのエージェント。<br>
./移動モデルの強化学習/qtable.py-->各エージェントが共有するQテーブルを設定する。<br>
./移動モデルの強化学習/visualization.py-->強化学習を行った結果を動画に変換するプログラム。<br>

## 実行環境
・python 3.8.8<br>
・モジュール  requirements.txtを参照

## 実行手順
./事前データの準備/ にロケーション履歴.jsonを配置<br>

python json2db.py でプログラムを実行<br>

./事前データの準備/data.db を./移動モデルの逆強化学習/ 内にコピー<br>

./移動モデルの逆強化学習/start.batを起動する<br>

start.bat<br>

実験名を入力しEnterを押すと処理が開始される(処理にリソースを取られるため、作業中のファイルを保存すること)<br>

result内に実験名のフォルダが生成され中に各時間での結果が保存る。<br>
各時間のフォルダからexcelファイルを抜き出し１つのフォルダに移動する。<br>

1つにまとめたフォルダを ./移動モデルの強化学習/input_data に移動する。<br>
./移動モデルの強化学習/network.py内の20行目<br>
wb = openpyxl.load_workbook("./input_data/ex_01_exe_fs/result"+str(n)+".xlsx")<br>
の./input_data/"ここをフォルダ名に書き換える"/result"+str(n)+".xlsx<br>
"ここをフォルダ名に書き換える"を指定したいフォルダ名に書き換えてから、./移動モデルの強化学習/main.dbを削除し各時間の移動経路と報酬を計算する<br>
python network.py  でプログラムを実行<br>

計算した経路を用いて強化学習を行う<br>
python main.py   でプログラムを実行<br>

結果の可視化を行う<br>
./移動モデルの強化学習/visualization.py内の115行目<br>
``` wb = openpyxl.load_workbook("./input_data/ex_01_exe_fs/result"+str(h)+".xlsx")``` <br>
内の ex_01_exe_fs の部分を可視化したいフォルダ名に書き換え<br>

python visualization.py でプログラムを実行<br>

結果を可視化した動画が、./移動モデルの強化学習/result内に保存される。<br>

以上で実験は終了となる。<br>



## 実行結果
実験を行い得られた例を./resultファイル内に保存しました。<br>
例として保存した２つのフォルダでは、報酬をθとしたものと報酬をθ*fsとしたものです。<br>
このパラメータの変更の仕方は、README.mdの21~25行目のように変更可能です。<br>
２つの実験では、stepreward.pngで1エピソードを1440ステップとしたときの累計報酬をプロットしました。<br>

## 注意
本プログラムを実行することによってgoogleロケーション履歴の結果がデータベース内に保存され個人情報の流出につながる可能性があるため注意してください。